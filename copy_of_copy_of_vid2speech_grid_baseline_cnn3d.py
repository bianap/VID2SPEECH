# -*- coding: utf-8 -*-
"""Copy of Copy of VID2SPEECH_GRID_baseline_CNN3D.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Ic5YdTywptEydg22DMVg7hXnpWvyLIR

in order to run deep learning experiments on GPU, you will have to change the runtime to GPU this way:

top menu: Runtime / Change runtime type / Hardware accelerator: GPU
"""

# install requirements:
#  SPTK for speech synthesis,
#  SoX for sound conversion,
#  PySPTK for spectrogram
!apt install sptk
!apt install sox
!pip3 install pysptk

# mount my own google drive
from google.colab import drive
drive.mount('/content/gdrive')

# check contents of my google drive
import os
os.chdir("/content/gdrive/My Drive/")
!ls -al

# create new folder for output
dir_output = "/content/gdrive/My Drive/deep_learning_VID2SPEECH/"
if not os.path.exists(dir_output):
  os.mkdir(dir_output)
os.chdir(dir_output)
!pwd

"""---

functions that will be used later
"""

# vocoder for synthesis

# vocoder_SPTK.py

from subprocess import run
import numpy as np
import scipy.io.wavfile as io_wav


def encode(basefilename, Fs_new = 11050, frlen = 512, frshft = 200, order = 12, alpha = 0.42, stage = 3, minF0 = 50, maxF0 = 400):

    # from HTS Makefile
    # $(X2X) +sf $${raw} | $(PITCH) -H $(UPPERF0) -L $(LOWERF0) -p $(FRAMESHIFT) -s $${SAMPKHZ} -o 2 > lf0/$${base}.lf0
    #
    # $(X2X) +sf $${raw} | \
    # $(FRAME) -l $(FRAMELEN) -p $(FRAMESHIFT) | \
    # $(WINDOW) -l $(FRAMELEN) -L $(FFTLEN) -w $(WINDOWTYPE) -n $(NORMALIZE) | \
    # $(MGCEP) -a $(FREQWARP) -c $(GAMMA) -m $(MGCORDER) -l $(FFTLEN) -e 1.0E-08 -o 4 | \
    # $(LPC2LSP) -m $(MGCORDER) -s $${SAMPKHZ} $${GAINOPT} -n $(FFTLEN) -p 8 -d 1.0E-08 > mgc/$${base}.mgc; \

    # calculate MGC-LSP
    command = 'sox "' + basefilename + '.wav"' + ' -t raw -r ' + str(Fs_new) + ' - ' + ' | sptk x2x +sf | ' + \
              'sptk frame -l ' + str(frlen) + ' -p ' + str(frshft) + ' | ' + \
              'sptk window -l ' + str(frlen) + ' -L ' + str(frlen) + ' -w 0 -n 1 | ' + \
              'sptk mgcep -a ' + str(alpha) + ' -c ' + str(stage) + ' -m ' + str(order) + ' -l ' + str(frlen) + ' -e 1.0E-08 -o 4 | ' + \
              'sptk lpc2lsp -m ' + str(order) + ' -s ' + str(Fs_new / 1000) + ' -n ' + str(frlen) + ' -p 8 -d 1.0E-08 > "' + basefilename + '.mgclsp"'
    # print(command)
    run(command, shell=True)

    # estimate pitch using SWIPE
    command = 'sox "' + basefilename + '.wav"' + ' -t raw -r ' + str(Fs_new) + ' - ' + ' | sptk x2x +sf | ' + \
              'sptk pitch -a 1 -H ' + str(maxF0) + ' -L ' + str(minF0) + ' -p ' + str(frshft) + ' -s ' + str(Fs_new / 1000) + ' -o 2 > "' + basefilename + '.lf0"'
    # print(command)
    run(command, shell=True)

    # read files for output
    mgc_lsp_coeff = np.fromfile(basefilename + '.mgclsp', dtype=np.float32).reshape(-1, order + 1)
    lf0 = np.fromfile(basefilename + '.lf0', dtype=np.float32)

    return (mgc_lsp_coeff, lf0) 

def decode(mgc_lsp_coeff, lf0, basefilename_out, Fs = 11050, frlen = 512, frshft = 200, order = 12, alpha = 0.42, stage = 3):
    
    # from HTS Training.pl / gen_wave
    #
    # MGC-LSPs -> MGC coefficients
    # $line = "$LSPCHECK -m " . ( $ordr{'mgc'} - 1 ) . " -s " . ( $sr / 1000 ) . " $lgopt -c -r 0.1 -g -G 1.0E-10 $mgc | ";
    # $line .= "$LSP2LPC -m " . ( $ordr{'mgc'} - 1 ) . " -s " . ( $sr / 1000 ) . " $lgopt | ";
    # $line .= "$MGC2MGC -m " . ( $ordr{'mgc'} - 1 ) . " -a $fw -c $gm -n -u -M " . ( $ordr{'mgc'} - 1 ) . " -A $fw -C $gm " . " > $gendir/$base.c_mgc";
    # shell($line);
    #
    # $line = "$SOPR -magic -1.0E+10 -EXP -INV -m $sr -MAGIC 0.0 $lf0 > $gendir/${base}.pit";
    #
    # $line = "$EXCITE -n -p $fs $gendir/$base.pit | ";
    # $line .= "$DFS -b $lfil | $VOPR -a $gendir/$base.unv | ";
    # $line .= "$MGLSADF -P 5 -m " . ( $ordr{'mgc'} - 1 ) . " -p $fs -a $fw -c $gm $mgc | ";
    # $line .= "$X2X +fs -o > $gendir/$base.raw";
    # shell($line);
    
    # write files for SPTK
    mgc_lsp_coeff.astype('float32').tofile(basefilename_out + '.mgclsp')
    lf0.astype('float32').tofile(basefilename_out + '.lf0')
    
    # MGC-LSPs -> MGC coefficients
    command = 'sptk lspcheck -m ' + str(order) + ' -s ' + str(Fs / 1000) + ' -c -r 0.1 -g -G 1.0E-10 "' + basefilename_out + '.mgclsp"' + ' | ' + \
              'sptk lsp2lpc -m '  + str(order) + ' -s ' + str(Fs / 1000) + ' | ' + \
              'sptk mgc2mgc -m '  + str(order) + ' -a ' + str(alpha) + ' -c ' + str(stage) + ' -n -u ' + \
                      '-M '  + str(order) + ' -A ' + str(alpha) + ' -C ' + str(stage) + ' > "' + basefilename_out + '.mgc"'
    # print(command)
    run(command, shell=True)
    
    # MGLSADF synthesis based on pitch and MGC coefficients
    command = 'sptk sopr -magic -1.0E+10 -EXP -INV -m ' + str(Fs) + ' -MAGIC 0.0 "' + basefilename_out + '.lf0"' + ' | ' + \
              'sptk excite -n -p ' + str(frshft) + ' | ' + \
              'sptk mglsadf -P 5 -m ' + str(order) + ' -p ' + str(frshft) + ' -a ' + str(alpha) + ' -c ' + str(stage) + ' "' + basefilename_out + '.mgc"' + ' | ' + \
              'sptk x2x +fs -o | sox -c 1 -b 16 -e signed-integer -t raw -r ' + str(Fs) + ' - -t wav -r ' + str(Fs) + ' "' + basefilename_out + '.wav"'
    # print(command)
    run(command, shell=True)
    
    # read file for output
    (Fs_out, x_synthesized) = io_wav.read(basefilename_out + '.wav')
    
    return x_synthesized

# from LIP_SSI_DNN_GRID.py

# required modules
import numpy as np
import matplotlib.pyplot as plt
import scipy.io.wavfile as io_wav
import os
import os.path
import gc
import re
import csv
import datetime
import scipy
import pickle
import cv2

from keras.models import Sequential
from keras.layers import Dense
from keras.callbacks import EarlyStopping, CSVLogger

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA, IncrementalPCA

# from LipReading
################## VIDEO INPUT WITH DIFF ##################
def load_video_3D(path):
    
    cap = cv2.VideoCapture(path)
    frameCount = THRESH_FRAME_COUNT
    frameHeight=int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT ))
    frameWidth=int(cap.get(cv2.CAP_PROP_FRAME_WIDTH ))

    buf =np.empty((frameHeight, frameWidth, frameCount), np.dtype('float32'))
    fc = 0
    ret = True
    
    while (fc < frameCount  and ret):
        ret, frame = cap.read()
        frame=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        frame=frame.astype('float32')
        #frame = frame-np.mean(frame)
        #frame = frame/np.std(frame)
        frame = frame-np.amin(frame)
        frame = frame/np.amax(frame)
        buf[:,:,fc]=frame
        fc += 1
    cap.release()

    return buf

def get_mgc_lsp_coeff(basefilename):
    if os.path.isfile(basefilename + '.mgclsp'):
        mgc_lsp_coeff = np.fromfile(basefilename + '.mgclsp', dtype=np.float32).reshape(-1, order + 1)
        lf0 = np.fromfile(basefilename + '.lf0', dtype=np.float32)
    else:
        Fs_new = 8000
        (mgc_lsp_coeff, lf0) = encode(basefilename, Fs_new, frameLength, frameShift, order, alpha, stage)
    return (mgc_lsp_coeff, lf0)

# download data and extract
if not os.path.isfile('GRID_s3_cut.tar.gz'):
  !wget http://smartlab.tmit.bme.hu/csapo/GRID/GRID_s3_cut.tar.gz
  !tar -xzvf GRID_s3_cut.tar.gz
!ls -al GRID/

# TODO: modify this according to your data path
speaker = 's3'
dir_video = dir_output + 'GRID/Video/' + speaker + '/face/'
dir_audio = dir_output + 'GRID/Audio/' + speaker + '/'

# Parameters of vocoder
samplingFrequency = 8000
frameLength = 1024 # 93 ms at 22050 Hz sampling
frameShift = 320 # 40 ms at 8000 Hz sampling, correspondong to 25 fps (lip video)
order = 12
alpha = 0.42
stage = 3
THRESH_FRAME_COUNT = 75

n_files = 100
n_file = 0
n_max_lip_frames = n_files * THRESH_FRAME_COUNT
n_lines = 128
n_pixels = 128
n_lines_reduced = 128
n_pixels_reduced = 128
n_mgc = order + 1
n_lip = n_lines_reduced * n_pixels_reduced
n_pca = 200
lip = np.empty((n_max_lip_frames, n_lip))
mgc = np.empty((n_max_lip_frames, n_mgc))
lip_size = 0
mgc_size = 0



if os.path.isdir(dir_video):
    for file in sorted(os.listdir(dir_video)):
        if ".avi" in file and n_file < n_files: # and "0088" in file:
            try:
                lip_data = load_video_3D(dir_video + file)
                (mgc_lsp_coeff, lf0) = get_mgc_lsp_coeff(dir_audio + file[:-4])
            except ValueError as e:
                print("wrong data, check manually!", e)
            
            else:
                n_file += 1
                
                mgc_lip_len = np.min((lip_data.shape[2], len(mgc_lsp_coeff)))
                
                lip_data = lip_data[:, :, 0:mgc_lip_len]
                mgc_lsp_coeff = mgc_lsp_coeff[0:mgc_lip_len]
                
                print(file[:-4], lip_data.shape, mgc_lsp_coeff.shape)
                
                # resize
                lip_len = lip_data.shape[2]
                if lip_size + lip_len > n_max_lip_frames:
                    raise
                for i in range(lip_len):
                    # lip[lip_size + i] = scipy.misc.imresize(lip_data[i], (n_lines_reduced, n_pixels_reduced), interp='bicubic') # 64x64
                    lip[lip_size + i] = lip_data[:, :, i].ravel() # original, 128x128
                    mgc[mgc_size + i] = mgc_lsp_coeff[i]

                lip_size += lip_len
                mgc_size += lip_len
                
                print('n_frames_all: ', lip_size, 'mgc_size: ', mgc_size)
                # calculate mgc features

lip = lip[0 : lip_size]
mgc = mgc[0 : mgc_size]

print(lip.shape, mgc.shape)
period = 3
samples = np.zeros((lip.shape[0]), dtype=[('input', float, (period ,128, 128)), ('output', float, (13))])

for i in range(0, len(lip)-period+1):
    temp_in = np.zeros((period, 128, 128))
    for j in range(0, period):
        temp_in[j] = lip[i+j].reshape(n_lines, n_pixels)/255    
    samples[i] = temp_in, mgc[i]

print(samples['output'].shape)
print(samples[0]['output'].shape)
print(samples[0]['output'])
print(samples[1]['input'])

train_samples = np.zeros(int(0.9*lip_size), dtype=[('input', float, (period, 128, 128)), ('output', float, (13))])
valid_samples = np.zeros(int(0.1*lip_size), dtype=[('input', float, (period, 128, 128)), ('output', float, (13))])

# use 90% of data for training, 10% for validation
train_samples = samples[0 : int(0.9*lip_size)]
valid_samples = samples[int(0.9*lip_size) : ]
#mgc_training = mgc[0 : int(0.9*mgc_size)]
#mgc_validation = mgc[int(0.9*mgc_size) : ]

# Resizes the inputs of the datasets
#train_x = np.reshape(train_samples['input'], (len(train_samples),3, 128, 128, 1))
#valid_x = np.reshape(valid_samples['input'], (len(valid_samples),3, 128, 128, 1))

from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Reshape
from keras.layers import TimeDistributed
model = Sequential()
model.add(Conv2D(input_shape=(3,128, 128), filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation="relu", use_bias=True, data_format= "channels_first"))
model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1, 1), padding='same', activation="relu", use_bias=True, data_format= "channels_first"))
model.add(MaxPooling2D(pool_size=(2, 2),  padding='same', data_format= "channels_first"))
model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1, 1), padding='same', activation="relu", use_bias=True, data_format= "channels_first"))
model.add(MaxPooling2D(pool_size=(2, 2),  padding='same', data_format= "channels_first"))
model.add(Flatten())
model.add(Dense(13))
#model.add(Reshape((-1, period, 13,1 )))
#model.add(TimeDistributed(Dense(13)))

#model.add(Dense(13, activation="tanh"))

# compile model
model.compile(loss='mean_squared_error', optimizer='adam')

print(model.summary())

# early stopping to avoid over-training
# csv logger
current_date = '{date:%Y-%m-%d_%H-%M-%S}'.format( date=datetime.datetime.now() )
print(current_date)
callbacks = [EarlyStopping(monitor='val_loss', patience=5, verbose=0), \
             CSVLogger(dir_output + 'VID2SPEECH_GRID_baseline_' + current_date + '.csv', append=True, separator=';')]

# Run training
history = model.fit(train_samples['input'], train_samples['output'],
                        epochs = 10, batch_size = 128, shuffle = True, verbose = 1,
                        validation_data=(valid_samples['input'], valid_samples['output']),
                        callbacks=callbacks)

model_json = model.to_json()
with open(dir_output + 'VID2SPEECH_GRID_baseline_' + current_date + '_model.json', "w") as json_file:
    json_file.write(model_json)

# serialize weights to HDF5
model.save_weights(dir_output + 'VID2SPEECH_GRID_baseline_' + current_date + '_weights.h5')

# serialize scalers to pickle
#pickle.dump(lip_pca, open(dir_output + 'VID2SPEECH_GRID_baseline_' + current_date + '_lip_pca.sav', 'wb'))
#pickle.dump(lip_scaler, open(dir_output + 'VID2SPEECH_GRID_baseline_' + current_date + '_lip_scaler.sav', 'wb'))
#pickle.dump(mgc_scalers, open(dir_output + 'VID2SPEECH_GRID_baseline_' + current_date + '_mgc_scalers.sav', 'wb'))



# evaluate on test data, from the end of file list
n_files = 2
n_file = 0
if os.path.isdir(dir_video):
    for file in sorted(os.listdir(dir_video)):
        if ".avi" in file and n_file < n_files and "00099" in file:
            print('testing on: ', file)
            lip_test = np.empty((75, n_lip))
            lip_data = load_video_3D(dir_video + file)
            lip_len = lip_data.shape[2]
            (mgc_lsp_coeff, lf0) = get_mgc_lsp_coeff(dir_audio + file[:-4])
            for i in range(lip_len):
                # lip[lip_size + i] = scipy.misc.imresize(lip_data[i], (n_lines_reduced, n_pixels_reduced), interp='bicubic') # 48x64
                lip_test[i] = lip_data[:, :, i].ravel() # original, 128x128
            

            #lip_test = lip_scaler.transform(lip_test)
            print(lip_test.shape)
            lip_test_shape = lip_test.shape
            input_test_data = np.zeros((lip_test_shape[0], 128, 128))
            
            for i in range(0, lip_test_shape[0]):
                input_test_data[i] = lip_test[i].reshape(n_lines, n_pixels)
            
            test_x = input_test_data.reshape((lip_test_shape[0], 128, 128, 1))
            #lip_test = np.reshape(lip_test, (-1,1,200))
            # predict using the trained model
            mgc_predicted = model.predict(test_x)
           # mgc_predicted_ = np.zeros((lip_test_shape[0], 13))
            #for i in range(n_mgc):
             #   mgc_predicted_[:, i] = mgc_scalers[i].inverse_transform(mgc_predicted[:, 0, i].reshape(-1, 1)).ravel()
                    
            n_file += 1
            
            lf0_whisper = np.ones((len(lf0))) * -1e10      
            print(mgc_predicted.shape)
            print(mgc_predicted[0].shape)
            print(mgc_predicted[29])
            #decode(mgc_lsp_coeff, lf0, dir_output + file[:-4] + '_original_vocoded', Fs = 8000, frlen = frameLength, frshft = frameShift, order = order, alpha = alpha, stage = stage)
          #  decode(mgc_predicted, lf0, dir_output + file[:-4] +'_' +current_date + '_CNN_predicted', Fs = 8000, frlen = frameLength, frshft = frameShift, order = order, alpha = alpha, stage = stage)
            #decode(mgc_lsp_coeff, lf0_whisper, dir_output + file[:-4] + '_original_vocoded_whisper', Fs = 8000, frlen = frameLength, frshft = frameShift, order = order, alpha = alpha, stage = stage)
          #  decode(mgc_predicted, lf0_whisper, dir_output + file[:-4] +'_' + current_date + '_CNN_predicted_whisper', Fs = 8000, frlen = frameLength, frshft = frameShift, order = order, alpha = alpha, stage = stage)













# listen to original sentence
import IPython

(Fs, x) = io_wav.read(dir_output + '000990_original_vocoded.wav')
IPython.display.Audio(x, rate=Fs)

# listen to synthesized sentence (using LIP video as input)
(Fs, x) = io_wav.read(dir_output + '000990_DNN_predicted_PCA.wav')
IPython.display.Audio(x, rate=Fs)

# listen to original sentence (whispered)
import IPython

(Fs, x) = io_wav.read(dir_output + '000990_original_vocoded_whisper.wav')
IPython.display.Audio(x, rate=Fs)

# listen to synthesized sentence (whispered)
(Fs, x) = io_wav.read(dir_output + '000990_2019-03-14_06-47-15_CNN_predicted_whisper.wav')
IPython.display.Audio(x, rate=Fs)

# save file to local drive
from google.colab import files
files.download(dir_output + '000990_DNN_predicted_PCA_whisper.wav')

# show original and predicted spectrograms

import pysptk
import librosa
from librosa import display

mgc_original = np.fromfile(dir_output + '000990_2019-03-14_06-47-15_CNN_predicted_whisper.mgc', dtype=np.float32).reshape(-1, order + 1)
logH_original = pysptk.mgc2sp(np.float64(mgc_original), 0.0, 0.0, frameLength).real
display.specshow(logH_original.T, sr=Fs, hop_length=frameShift, x_axis="time", y_axis="linear")

mgc_predicted = np.fromfile(dir_output + '000990_DNN_predicted_PCA_whisper.mgc', dtype=np.float32).reshape(-1, order + 1)
logH_predicted = pysptk.mgc2sp(np.float64(mgc_predicted), 0.0, 0.0, frameLength).real
display.specshow(logH_predicted.T, sr=Fs, hop_length=frameShift, x_axis="time", y_axis="linear")

